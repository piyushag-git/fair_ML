### Fairness in Machine Learning

Arthur Samuel coined the term “Machine Learning” in 1959 by demonstrating how artificial learning systems can perform better than their human counterpart in a game of checkers. Fast forward to the present, 97% of companies are using or planning to use machine learning in some form to scale and automate their business. ML-based applications such as recommendation systems, image recognition, classification systems, and helpline automation are widely used by businesses in multiple industries. Due to this “boom” of machine learning adoption, new algorithms, and platforms are emerging that aim to be more accurate and scalable. One caveat, however, is the reliance of machine learning systems on the nature of the data. The data which is used to train the model is susceptible to various problems such as data imbalance, noisy data, and data sparsity to name a few. If the model is trained on such datasets, it is prone to be less accurate and/or biased.
Bias can take many forms. Since machine learning models do not have the ability to actually understand anything, they can exhibit discriminatory behavior and give misleading results. A cause for such behavior is training the model on historical data which may have an inherent pattern of historical unfairness that the model learns during training. The European Union has regulations in place like the “right to explanation” which states that any significant decision cannot be solely based on automated systems. This is justified as there is a trend of increasing use of machine learning in various industries to automate many workflows.
Fairness-based machine learning modeling is one of the most prominent research areas today. These models use different techniques such as data reweighing, data resampling, omitting the sensitive attributes, etc. that complement the data and the model to mitigate the issue of bias in the model. In addition to the standard performance metrics such as accuracy, precision, recall, etc. various algorithmic fairness metrics are also used to calculate bias in the models. This project focuses on how standard models fare against fairness-based models and proposes some data augmentation methods to counter the bias in machine learning model.
